% -------------------------------------------------------
% Results
% -------------------------------------------------------

\section{Results}

Meta-analyses of eye movements are relatively rare, potentially because of methodological challenges in combining effect sizes from different eye-tracking studies. Two main challenges are how to handle measurement validity across eye-tracker types and how to compare different eye movement dependent variables. To handle these issues, we developed correction procedures integrated in a psychometric meta-analysis \citep{hunter2004a}, which allow us to quantify the interference of measurement validity or multiple metrics. The measurement validity issue stems from differences in the accuracy and precision of eye-tracking equipment \citep{holmqvist2015a}, which can affect the data quality and bias effect sizes \citep{orquin2016a}. \chg{}{We developed a correction method that relies on an empirical estimate of the relationship between eye-tracker characteristics and observed effect sizes (see \textit{Methods}; Figure~\ref{fig:ET_accuracy_effectsize}; Table~\ref{tab:eyetracker_specifications}).} Most metrics are based on fixations -- defined as maintaining the gaze on a single location or area of interest (AOI), such as fixation count, fixation likelihood, total fixation duration and so on. This leads to a potential issue with comparing effect sizes reported with different dependent variables. We developed a correction method that makes the dependent variables comparable, where we empirically estimate correction factors based on a subset of studies in our sample that report multiple dependent variables (see \textit{Methods}; Figure~\ref{fig:metric_correction}; Table~\ref{tab:metric_correction}). This method allowed us to transform all effect sizes to a single metric; we decided for fixation count which was used in all meta-analyses. 

In what follows, we first analyse the group of visual factors and then the group of cognitive factors. We perform meta-analysis on each individual factor  separately. We next perform a small moderator analysis and finish with an analysis of publication bias in all the meta-analyses.  


\subsection{Visual factors}

\chg{results-new-visual-factors}{We focused on four major groups of visual factors -- salience, position, surface size and set size (see \textit{Methods} for coding procedure). The summary effects of the visual factors on attention during decision making show that the factors range from small to medium effect sizes (see Table~\ref{tab:main_results} and Figure~\ref{fig:forest_plots_visual}). The trim and fill correction lead to adjustments only for left vs right position and set size. The left vs right position effect is corrected to a near null effect, but the small sample size, $k = 5$, casts some doubt on the precision of this correction. Overall, the trim and fill correction suggests that there is little publication bias in the visual factors. Inspecting the effect sizes, it is interesting that salience, which so far has taken the center stage in vision science, has only a small summary effect. The largest effects are those of center position, $.43$, and surface size, $.35$, which are larger than the trim and fill corrected cognitive factors which range between $.26-.41$. While it is possible to eliminate the effects of, for instance, center position and surface size in laboratory studies, this is not true for natural environments. In natural environments it is reasonable to expect that multiple, and most likely all, visual factors influence eye movements at the same time. Not only does the effects of the largest visual factors exceed the largest cognitive factors when comparing one by one, but considering their joint effect we believe it is reasonable to conclude that visual factors play a larger role than cognitive factors in determining visual attention in decision making.} 

\input{tables/main_results.tex}

\begin{figure}[!h]
\includegraphics{forest_plots_visual}
\centering
\caption{Effect sizes of the visual factors are moderate, except for salience and left-vs-right position, which have small effect sizes, if any. Forest plots show the unattenuated effect size correlations for each study in a group, as well as average effect across the group. Forest plot in (A) shows the effect sizes for salience factor, in (B) for center position, in (C) for left vs right position, in  (D) for surface size, and in (E) for set size factor. Error bars represent the 95\% confidence interval around the mean.}
\label{fig:forest_plots_visual}
\end{figure}

\subsection{Cognitive factors}

Previous research has identified a wide range of cognitive factors that influence attention, such as goals, task instructions, and preferences \citep[for a review see][]{orquin2013a}. Here, we divided cognitive control factors into three groups: task instruction, preferential viewing and choice bias. In studies on task instructions, participants receive instructions concerning a specific decision goal, and with that, what is relevant to gaze at. For instance, the participants may be instructed on the validity of stimulus attributes \citep{krefeld-schwalb2019a}, or infer the level of validity themselves \citep{bialkova2014a}. In preferential viewing studies, the relevance should be equal to the subjective preferences. For example, some alternatives have higher subjective values than others \citep{kim2012a}. Because of this qualitative difference between the two domains, we treated studies on task instructions and preferential viewing separately. 
\chg{cognitive fac}{Inspecting the effect sizes reveals that the summary effects in the two types of studies are moderate and similar in magnitude (see Table~\ref{tab:main_results} and Figure~\ref{fig:forest_plots_cognitive}). Using a Wald test, we find that effect sizes of task instructions and preferential viewing are unlikely to differ, \input{./tables/difftest_task_pref.tex}. 
The test suggests that it makes no difference to eye movements whether the relevance of information is defined according to an externally specified goal or according to preferences. Note however, that the two effect sizes differ more when comparing the trim and fill adjusted effect sizes.}

Choice bias refers to an effect in attention whereby decision makers spend more time gazing at the eventually chosen alternative. This effect, originally introduced by Shimojo and colleagues \citep{shimojo2003a} as a ``gaze-cascade'' effect, is well-established in the literature, prompting us to study it as a separate factor. This factor consists of studies reporting the difference in eye movements between the chosen alternative and all other (not chosen) alternatives. \chg{cognitive fac 2}{We find that choice bias has a large effect on eye movements which however is reduced to a medium effect, $.41$, after the trim and fill adjustment.}


\begin{figure}[!h]
\includegraphics{forest_plots_cognitive}
\centering
\caption{Effect sizes of the three cognitive factors are moderate to large. Forest plots show the unattenuated effect size correlations for each study in a group, as well as average effect across the group. Forest plot in (A) shows the effect sizes for task instructions factor, in (B) for preferential viewing, and in (C) for the choice bias factor. Error bars represent the 95\% confidence interval around the mean.}
\label{fig:forest_plots_cognitive}
\end{figure}


\subsection{Moderator analyses}

Alternatives that participants in judgment and decision making studies choose between can often be decomposed into constituent elements, commonly called attributes, cues or features \citep{payne1988,tversky1972elimination,stojic2020s,gigerenzer1996reasoning,schulz2018putting,hogarth2007heuristic}. For example, in classical lottery tasks \citep{tversky1979}, the probabilities and values of an alternative can be viewed as attributes. Or, in multi-cue judgment tasks, alternatives are more explicitly composed of cues -- university, major football team or main city in the German city size task \citep{gigerenzer1996reasoning}. This has consequences for both modelling of decision processes and units of analysis. Consequently, some studies in our sample focused on attention effects at either alternative or attribute level, or both. This was in particular the case for studies involving set size, task instructions, and preferential viewing factors. Since the alternative vs attribute dimension might be an important moderator in these groups, we decomposed them further with regards to the effect of alternatives vs attributes (Table~\ref{tab:mod_results} and Figure~\ref{fig:forest_plots_altatt}). Moderator analyses shows support for the alternative vs attribute moderator across set size, \input{./tables/moderator_setsize.tex}, but no support for preferential viewing, \input{./tables/moderator_pref.tex}, or for task instructions, \input{./tables/moderator_task.tex}. It is worth noting that effect sizes are consistently larger when operationalized at the level of alternatives compared to attributes (Table~\ref{tab:mod_results} and Figure~\ref{fig:forest_plots_altatt}). 

We also performed a moderator analysis for the choice bias factor, to assess whether the effect is driven by preferential viewing as proposed by \cite{shimojo2003a}. We compare studies with preferential vs inferential choice tasks and find no support for moderation by decision type, \input{./tables/moderator_choicebias.tex}, and therefore only report results for the main group. 


% -------------------------------------------------------
% Publication bias
% -------------------------------------------------------

\subsection{Publication bias}

\chg{PB-PET}{We assessed potential publication bias by first performing a precision-effect test \citep{stanley2014} on the Fisher z transformed effect sizes and variances. This test uses ordinary least squares to regress individual study effect sizes on study standard deviations weighted by the study precision. It is not recommended to use the test in case of large heterogeneity or on small samples, e.g. less than 10 studies \citep{vanaert2019}, and we therefore use it on the complete data set. We control for the artifact multiplier since the eye-tracker accuracy plays an important role in determining effect sizes and thus contribute to heterogeneity. The PET shows a significant effect of standard deviation of the effect size, suggesting the presence of publication bias (see Table \ref{tab:PET}).}

\input{tables/PET}


\chg{PB-PEESE}{Given the significant PET, we then perform the precision-effect estimate with standard errors test (PEESE). The PEESE differs only in using the study variance instead of the standard deviation. The intercept in the PEESE is normally used as the publication bias corrected estimate (see Table \ref{tab:PEESE}). The bias corrected estimate in the PEESE test is, in our case, not very informative by itself since it groups all independent variables into one estimate. However, by comparing the PEESE intercept to that obtained with from an intercept only regression, \input{tables/FE}\unskip, we can get an impression of the degree of effect size overestimation due to publication bias. This comparison suggests that publication bias leads to an overestimation factor of \input{tables/peeseFactor}\unskip.} 

\input{tables/PEESE}


\chg{PB-public-grant}{We perform an additional analysis of publication bias, because of the above mentioned limitations of the PET-PEESE test. It has been shown that studies which receive public grants are more likely to be published \citep{canestaro2017}, and we therefore expect that acknowledging a public grant in the article may be associated with smaller effect sizes. We use an inverse-variance random effects model with public grant as moderator and find that public grants have a marginally significant effect on effect sizes in the expected direction, \input{tables/publicSig}\unskip. Comparing public vs non-public funded research we derive an overestimation factor of \input{tables/publicFactor}\unskip.} 

\chg{PB-trim-fill}{The PET-PEESE and public grant analyses are based on very different premises, but they both suggest the presence of publication bias. We therefore proceed with a publication bias correction using the trim and fill analysis \citep{duval2000trim} at the level of each independent variable. The trim and fill analysis resulted in a downward adjustment of the average effect size for most of the independent variables. The corrected effect sizes in Table~\ref{tab:main_results} (in parentheses) provide a more conservative estimate of the true population effects, but are also subject to some uncertainty. Specifically, the interpretation of the corrected results may be biased due to heterogeneity in many of the factors as well as a relatively small number of studies in visual factor groups. Comparing the uncorrected with the trim and fill corrected estimates suggests and overestimation factor due to publication bias of \input{tables/trimFactor} which is within the range of the overestimation factors from the PET-PEESE and the public grant analysis, \input{tables/peeseFactor}\unskip-\input{tables/publicFactor}\unskip.} 

\chg{PB-interpretation}{The three analyses suggest an overestimation factor somewhere in the range of 1.3 to 1.4. While any factor above 1 is undesirable, it is worth noting it is far below the factor of 2.59 suggested by \cite{kvarven2020} who compared 15 pairs of meta-analyses estimates with estimates from many-lab replication studies. In addition to these analyses, we plotted the Fisher transformed correlation coefficients of each study by its respective standard error (so-called funnel plots; Figure~\ref{fig:funnel_plots} for main results, and Figure~\ref{fig:funnel_plots_altatt} for moderator analyses). The symmetry of the funnel plots provides a qualitative picture of publication bias since we expect that studies with smaller sample sizes and hence higher standard errors yield more variable effect sizes, the smallest of which are less likely to be published, leading to an asymmetric funnel plot.}


% -------------------------------------------------------
% Descriptive EM analysis
% -------------------------------------------------------

\chg{descriptiveEM-overview}{%
%
\subsection{Descriptive eye movement data}
%
To better understand the results of the meta-analysis we extract for each effect size the corresponding descriptive eye movement data whenever the included study reports this information (see \textit{Methods}). Some studies report eye movement data across conditions, e.g. fixation count across high and low salience conditions, while others report data for each condition separately, e.g. fixation count for high vs low salience conditions. We extracted the descriptive eye movement data at the unit of an single AOI, e.g. corresponding to the fixation likelihood or fixation count for a single attribute level or for a single alternative if the AOI's are defined at this level. A total of \input{tables/authorEMcount} articles report descriptive eye movement data, resulting in \input{tables/EMcount} corresponding pairs of effect sizes and descriptive eye movement data, with some studies reporting results for more than one dependent variable. In total there are \input{tables/flEMcount} studies reporting fixation likelihood, \input{tables/fcEMcount} reporting fixation count, and \input{tables/tdtEMcount} reporting total dwell time.}


\begin{figure}[!h]
\includegraphics{figs/EMtoES.pdf}
\centering
\caption{Descriptive eye movement data provides insight on attention behaviour of an average participant and how would effect sizes translate to original measures they were based on. Distribution plots illustrate the fixation likelihood (A), the average fixation count (B), and the average total dwell time (C) across studies. We illustrate the linear relationship between the effect size correlation and the descriptive eye movement data by logit transforming differences in fixation likelihood (D), log transforming fixation count (E), and log transforming total dwell time (F).}
\label{fig:em_figure}
\end{figure}


\chg{descriptiveEM-results}{From averaged eye movement measures we see that the average for fixation likelihood is \input{tables/flmean}\unskip, for fixation count \input{tables/fcmean}\unskip, and total dwell time \input{tables/tdtmean}\unskip. From the distribution plots it is clear that there is a lot of variance in fixation likelihood across studies (Figure~\ref{fig:em_figure}). In some studies participants fixate nearly all AOI's, but there is also a large number of studies in which participants fixate half or less of the AOI's. Main usage of the descriptive eye movement data, however, is to provide intuitions about the synthesized effect sizes. To this end, we transform the synthesized effect size for each independent variable into its corresponding effect on fixation likelihood, fixation count and total dwell time (see \textit{Methods} for details). We achieve this transformation by regressing descriptive measure (appropriately transformed) on effect size correlations, using a linear mixed model with a random intercept, grouped by article to account for correlated errors. For fixation likelihood (logit difference between conditions) the model intercept is not significantly different from zero, whereas the slope is, \input{tables/FLtoLogitModel}\unskip. Figure \ref{fig:em_figure} panel (D) illustrates the relationship between the transformed statistic and effect size correlations, with an increasing variance as effect sizes become larger. For fixation count (log difference between conditions) the model shows a similar pattern where the intercept is not significantly different from zero, while the slope is, \input{tables/FCtoLogModel}\unskip, as illustrated in Figure~\ref{fig:em_figure} panel (E). The model for total dwell time (log difference between conditions) differs in having a significant intercept alongside a significant slope, \input{tables/TDTtoLogModel}\unskip, as illustrated in Figure~\ref{fig:em_figure} panel (F). Overall, all three measures strongly correlate with the (transformed) effect sizes, giving us confidence for converting effect sizes into original measures using the fitted models. More specifically, for each independent variable we compute expected increase in descriptive eye movement measure based on the effect size, for a study with an average descriptive measure (see \textit{Methods} for details). Combining the estimates from these operations we can finally compare the effect sizes for each independent variable in terms of the equivalent effect on fixation likelihood, fixation count, and total dwell time for an average study (see Table~\ref{tab:em_results}).}


\input{tables/em_results}