% -------------------------------------------------------
% Results
% -------------------------------------------------------

\section{Results}

Meta-analyses of eye movements are relatively rare, potentially because of methodological challenges in combining effect sizes from different eye-tracking studies. Two main challenges are how to handle measurement validity across eye-tracker types and how to compare different eye movement dependent variables. To handle these issues, we developed correction procedures integrated in a psychometric meta-analysis \citep{hunter2004a}, which allow us to quantify the interference of measurement validity or multiple metrics. The measurement validity issue stems from differences in the accuracy and precision of eye-tracking equipment \citep{holmqvist2015a}, which can affect the data quality and bias effect sizes \citep{orquin2016a}. We developed a correction method that relies on an empirical estimate of the relationship between eye-tracker characteristics and observed effect sizes (see \textit{Method}; Figure~\ref{fig:ET_accuracy_effectsize}; Table~\ref{tab:eyetracker_specifications}). Most metrics are based on fixations -- defined as maintaining the gaze at a single location or area of interest (AOI), such as fixation count, fixation likelihood, total dwell time, and so on. This leads to a potential issue with comparing effect sizes reported with different dependent variables. We developed a correction method that makes the dependent variables comparable, where we empirically estimate correction factors based on a subset of studies in our sample that report multiple dependent variables (see \textit{Method}; Figure~\ref{fig:metric_correction}; Table~\ref{tab:metric_correction}). This method allowed us to transform all effect sizes to a single metric; we decided for fixation count, which was used in all meta-analyses. 

To understand the context in which the studies were conducted, we coded participant sample information -- age, gender, ethnicity, and country -- across visual and cognitive factors (Table~\ref{tab:sampleTable} in the Appendix). Generally, participant samples include a roughly even split of men and women in many cases with a mean age above 30. The studies are collected in a wide range of countries, but are with few exceptions restricted to white participants. 

In what follows, we first analyze the group of visual factors and then the group of cognitive factors. We perform a meta-analysis on each individual factor  separately. We next perform a small moderator analysis and finish with an analysis of publication bias in all the meta-analyses.


\subsection{Almost all visual factors have substantial effect sizes}

We focused on four major groups of visual factors -- salience, position, surface size, and set size (see \textit{Method} for coding procedure). The summary effects of the visual factors on attention during decision making show that the factors range from small to medium effect sizes (see Table~\ref{tab:main_results} and Figure~\ref{fig:forest_plots_visual}). Since the publication bias analyses suggest the presence of bias (see \textit{Publication bias} section below), we conducted a sensitivity analysis that indicates the possible extent of bias in individual factors. The Top10 sensitivity analysis (see \textit{Method} for details) suggests large upward adjustment for three factors -- salience, surface size, and center position -- resulting in medium to large effect sizes (Table~\ref{tab:main_results}; results in parentheses). For set size the Top10 analysis suggests no change in effect size, while for left vs. right position it suggests large downward correction to a near null effect. While the left vs. right position does not seem to have an effect on fixation count, we cannot exclude effects on temporal dynamics. For instance, decision makers may be more likely to begin with the left option and then move right \citep{fiedler2012}, which could lead to primacy effects on choice. Overall, the Top10 analysis suggests that there is some publication bias in the visual factors. Inspecting the effect sizes, it is interesting that salience, which so far has taken the center stage in vision science, has smaller summary effect than some of the other visual factors. The largest effects are those of center position, $r=.43$, and surface size, $r=.35$, which are on par with cognitive factors, which range between $r = .35$ and $r = .59$. If considering the Top10 analysis results, then visual factors are substantially larger than cognitive factors. While it is possible to eliminate the effects of, for instance, center position and surface size in laboratory studies, this is not true for natural environments. In natural environments it is reasonable to expect that multiple, and most likely all, visual factors influence eye movements at the same time. Not only do the effects of the largest visual factors exceed the largest cognitive factors when comparing one by one, but considering their joint effect we believe it is reasonable to conclude that visual factors play a larger role than cognitive factors in determining visual attention in decision making. 


\input{tables/main_results.tex}


\begin{figure}[!h]
\includegraphics{forest_plots_visual}
\centering
\caption{Effect sizes of the visual factors are moderate, except for salience and left vs. right position, which have small effect sizes, if any. Forest plots show the unattenuated effect size correlations for each study in a group, as well as average effect across the group. Forest plot in (A) shows the effect sizes for salience factor, in (B) for center position, in (C) for left vs. right position, in  (D) for surface size, and in (E) for set size factor. Error bars represent the 95\% confidence interval around the mean.}
\label{fig:forest_plots_visual}
\end{figure}


\subsection{Cognitive factors have effect sizes similar in magnitude to visual factors}

Previous research has identified a wide range of cognitive factors that influence attention, such as goals, task instructions, and preferences \citep[for a review see][]{orquin2013a}. Here, we divided cognitive control factors into three groups: task instruction, preferential viewing, and choice-gaze effect. In studies on task instructions, participants receive instructions concerning a specific decision goal, and with that, what is it relevant to gaze at. For instance, the participants may be instructed on the validity of stimulus attributes \citep{krefeld-schwalb2019a}, or infer the level of validity themselves \citep{bialkova2014a}. In preferential viewing studies, the relevance should be equal to the subjective preferences. For example, some alternatives have higher subjective values than others \citep{kim2012a}. Because of this qualitative difference between the two domains, we treated studies on task instructions and preferential viewing separately. 
Inspecting the effect sizes reveals that the summary effects in the two types of studies are moderate and similar in magnitude (see Table~\ref{tab:main_results} and Figure~\ref{fig:forest_plots_cognitive}). Using a Wald test, we find that effect sizes of task instructions and preferential viewing are unlikely to differ, \input{./tables/difftest_task_pref.tex}. 
The test suggests that it makes no difference to eye movements whether the relevance of information is defined according to an externally specified goal or according to preferences. Note, however, that the two effect sizes differ more when comparing the effect sizes from the Top10 analysis, which suggests downward adjustments, with larger adjustment for task instructions (Table~\ref{tab:main_results}; results in parentheses).

Choice-gaze effect refers to an effect in attention whereby decision makers spend more time gazing at the eventually chosen alternative. This effect, originally introduced by Shimojo and colleagues \citep{shimojo2003a} as a ``gaze-cascade'' effect, is well-established in the literature, prompting us to study it as a separate factor. This factor consists of studies reporting the difference in eye movements between the chosen alternative and all other (not chosen) alternatives. We find that choice-gaze effect has a large effect on eye movements, $r=.59$. However, the Top10 analysis suggests a large downward adjustment, to a small effect, $r=.26$.


\begin{figure}[!h]
\includegraphics{forest_plots_cognitive}
\centering
\caption{Effect sizes of the three cognitive factors are moderate to large. Forest plots show the unattenuated effect size correlations for each study in a group, as well as average effect across the group. Forest plot in (A) shows the effect sizes for task instructions factor, in (B) for preferential viewing, and in (C) for the choice-gaze effect factor. Error bars represent the 95\% confidence interval around the mean.}
\label{fig:forest_plots_cognitive}
\end{figure}


\subsection{Alternative vs. attribute moderator is significant only for the set size factor}

Alternatives that participants in judgment and decision making studies choose between can often be decomposed into constituent elements, commonly called attributes, cues, or features \citep{payne1988,tversky1972elimination,stojic2020s,gigerenzer1996reasoning,schulz2018putting,hogarth2007heuristic}. For example, in classical lottery tasks \citep{tversky1979}, the probabilities and values of an alternative can be viewed as attributes. Or, in multi-cue judgment tasks, alternatives are more explicitly composed of cues -- in the German city size task, for example, these would be university, major football team, main city, and so on \citep{gigerenzer1996reasoning}. This has consequences for both modelling of decision processes and units of analysis. Consequently, some studies in our sample focused on attention effects at either alternative or attribute level, or both. This was in particular the case for studies involving set size, task instructions, and preferential viewing factors. Since the alternative vs. attribute dimension might be an important moderator in these groups, we decomposed them further with regards to the effect of alternatives vs. attributes (Table~\ref{tab:mod_results} and Figure~\ref{fig:forest_plots_altatt}). Moderator analyses show support for the alternative vs. attribute moderator across set size, \input{./tables/moderator_setsize.tex}\unskip, but no support for preferential viewing, \input{./tables/moderator_pref.tex}\unskip, or for task instructions, \input{./tables/moderator_task.tex}\unskip. It is worth noting that effect sizes are consistently larger when operationalized at the level of alternatives compared to attributes (Table~\ref{tab:mod_results} and Figure~\ref{fig:forest_plots_altatt}). 

We also performed a moderator analysis for the choice-gaze effect factor, to assess whether the effect is driven by preferential viewing as proposed by \cite{shimojo2003a}. We compare studies with preferential vs. inferential choice tasks and find no support for moderation by decision type, \input{./tables/moderator_choicebias.tex}\unskip, and therefore only report results for the main group. 


% -------------------------------------------------------
% Publication bias
% -------------------------------------------------------

\subsection{Publication bias exists, but mainly affects cognitive factors}

We assessed potential publication bias by first performing a precision-effect test \citep[PET][]{stanley2014}, which regresses individual study effect sizes on study standard deviations weighted by the study precision, controlling for the eye-tracker accuracy in addition (see \textit{Method} for details). It is not recommended to use the test in case of small samples (for example, less than 10 studies \citep{vanaert2019}), and we therefore used it on the complete data set. Because many articles report more than one effect size we computed robust variance estimates of all model coefficients based on a sandwich-type estimator. While the PET is not significant, \input{tables/PETintext}\unskip, Egger's test shows a significant effect of standard deviation of the effect size, \input{tables/EGGERintext}\unskip, suggesting the presence of publication bias (see \textit{Appendix} Table \ref{tab:PET-PEESE} for full regression results). Given the significant Egger's test, we then performed the precision-effect estimate with standard errors test (PEESE). The PEESE differs only in using the study variance instead of the standard deviation. The intercept in the PEESE is normally used as the publication bias corrected estimate, \input{tables/PEESEintext} (see \textit{Appendix} Table \ref{tab:PET-PEESE} for full regression results). The bias corrected estimate in the PEESE test is, in our case, not very informative by itself since it groups all independent variables into one estimate. However, by comparing the PEESE intercept to that obtained from an intercept only regression, \input{tables/FE}\unskip, we can get an impression of the degree of effect size overestimation due to publication bias. This comparison suggests that publication bias leads to an overestimation factor of \input{tables/peeseFactor}\unskip.

We performed an additional analysis of publication bias because of the above-mentioned limitations of the PET-PEESE test. It has been shown that studies which receive public grants are more likely to be published \citep{canestaro2017}, and we therefore expected that acknowledging a public grant in the article may be associated with smaller effect sizes. We used a random effects model with robust variance estimation and public grant as moderator (see \textit{Method}). We find that although public grants are associated with smaller effect sizes, the moderator does not have a significant effect, \input{tables/publicSig}\unskip. Comparing public vs. non-public funded research we derived an overestimation factor of \input{tables/publicFactor}\unskip, which is nevertheless higher than the estimate derived from the PET-PEESE analysis.

The PET-PEESE and public grant analyses are based on very different premises, but they both indicate the presence of publication bias. We therefore proceeded with a Top10 analysis, an additional sensitivity analysis at the level of each independent variable (see \textit{Method}). The Top10 analysis resulted in an upward adjustment of the average effect size for several visual factors, downward adjustment for all cognitive factors and the visual factor left vs. right position. The corrected effect sizes in Table~\ref{tab:main_results} (in parentheses) provide a more conservative estimate of the true population effects, but are also subject to some uncertainty because of a relatively small number of studies in the visual factor groups. Comparing the Top10 with the uncorrected estimates suggests an overestimation factor of \input{tables/trimFactor}\unskip, which is close to the overestimation factors from the PET-PEESE and the public grant analysis.

The three analyses suggest an overestimation factor somewhere in the range of \input{tables/peeseFactor} to \input{tables/trimFactor}\unskip. While any factor above 1 is undesirable, it is worth noting that it is far below the factor of 2.59 suggested by \cite{kvarven2020} who compared 15 pairs of meta-analysis estimates with estimates from many-lab replication studies. In addition to these analyses, we plotted the Fisher transformed correlation coefficients of each study by its respective standard error (so-called funnel plots; Figure~\ref{fig:funnel_plots} for main results, and Figure~\ref{fig:funnel_plots_altatt} for moderator analyses). The symmetry of the funnel plots provides a qualitative picture of publication bias since we expect that studies with smaller sample sizes and hence higher standard errors yield more variable effect sizes, the smallest of which are less likely to be published, leading to an asymmetric funnel plot. Funnel plots are generally rather difficult to interpret, and in our case it is almost impossible to draw any conclusion for visual factors, given the smaller number of studies on which they are based.


% -------------------------------------------------------
% Descriptive EM analysis
% -------------------------------------------------------


\subsection{Projecting effect sizes to original measurement units reveals substantial effects in absolute sense}

To better understand the results of the meta-analysis we extract for each effect size the corresponding descriptive eye movement data whenever the included study reports this information (see \textit{Method}). For example, a study would report a fixation count (or likelihood or dwell time) for high vs. low salience conditions. We extracted the descriptive eye movement data at the unit of a single AOI, for example, corresponding to the fixation count for a single attribute level or for a single alternative if the AOIs are defined at this level. A total of \protect{\input{tables/authorEMcount}\unskip} articles report descriptive eye movement data, resulting in \protect{\input{tables/EMcount}\unskip} corresponding pairs of effect sizes and descriptive eye movement data, with some studies reporting results for more than one dependent variable. In total, there are \protect{\input{tables/flEMcount}\unskip} studies reporting fixation likelihood, \protect{\input{tables/fcEMcount}\unskip} reporting fixation count, and \protect{\input{tables/tdtEMcount}\unskip} reporting total dwell time.


\begin{figure}[!h]
\includegraphics{figs/EMtoES.pdf}
\centering
\caption{Descriptive eye movement data provides insight into the attention behavior of an average participant and how effect sizes translate into the original measures on which they were based. Distribution plots illustrate the fixation likelihood (A), the average fixation count (B), and the average total dwell time (C) across studies. We illustrate the linear relationship between the effect size correlation and the descriptive eye movement data by logit transforming differences in fixation likelihood (D), log transforming fixation count (E), and log transforming total dwell time (F). Lines are based on intercepts and slopes from linear mixed model fits (see main text and \textit{Method} for details).}
\label{fig:em_figure}
\end{figure}


From averaged eye movement measures we see that the average for fixation likelihood is \input{tables/flmean}\unskip, for fixation count \input{tables/fcmean}\unskip, and total dwell time \input{tables/tdtmean}\unskip. From the distribution plots it is clear that there is a lot of variance in fixation likelihood across studies (Figure~\ref{fig:em_figure}). In some studies participants fixate nearly all AOIs, but there is also a large number of studies in which participants fixate half or less of the AOIs. The main use of the descriptive eye movement data, however, was to provide intuitions about the synthesized effect sizes. To this end, we transformed the synthesized effect size for each independent variable into its corresponding effect on fixation likelihood, fixation count, and total dwell time (see \textit{Method} for details). We achieved this transformation by regressing descriptive measures (appropriately transformed) on effect size correlations, using a linear mixed model with a random intercept, grouped by article to account for correlated errors. For fixation likelihood (logit difference between conditions), the model intercept is not significantly different from zero, whereas the slope is, \input{tables/FLtoLogitModel}\unskip. Figure \ref{fig:em_figure} panel (D) illustrates the relationship between the transformed statistic and effect size correlations, with an increasing variance as effect sizes become larger. For fixation count (log difference between conditions), the model shows a similar pattern where the intercept is not significantly different from zero, while the slope is, \input{tables/FCtoLogModel}\unskip, as illustrated in Figure~\ref{fig:em_figure} panel (E). The model for total dwell time (log difference between conditions) reveals the same pattern, \input{tables/TDTtoLogModel}\unskip, as illustrated in Figure~\ref{fig:em_figure} panel (F). Overall, all three measures strongly correlate with the (transformed) effect sizes, giving us confidence for converting effect sizes into original measures using the fitted models. More specifically, for each independent variable we computed the expected increase in descriptive eye movement measures based on the effect size, for a study with an average descriptive measure (see \textit{Method} for details). Combining the estimates from these operations we could finally compare the effect sizes for each independent variable in terms of the equivalent effect on fixation likelihood, fixation count, and total dwell time for an average study (see Table~\ref{tab:em_results}). The table reveals surprisingly large effects on eye movements when expressing effect size correlations in raw eye movement metrics. Even the smallest effect observed for salience, $\rho = .13$, leads to an increase in fixation likelihood from an average of $61\%$ to $76\%$. Using a central location increases fixation likelihood to $91\%$. For fixation count the numbers are equally surprising. From an average fixation count of $7.8$, increasing salience can add three more fixations to a piece of information, while using a central location can add nearly ten more fixations. For the purpose of policy interventions aiming to capture decision maker attention, these numbers are beyond all our expectations.

\input{tables/em_results}
