% -------------------------------------------------------
% Results
% -------------------------------------------------------

\section{Results}

Meta-analyses of eye movements are relatively rare, potentially because of methodological challenges in combining effect sizes from different eye-tracking studies. Two main challenges are how to handle measurement validity across eye-tracker types and how to compare different eye movement dependent variables. To handle these issues, we developed correction procedures integrated in a psychometric meta-analysis \citep{hunter2004a}, which allow us to quantify the interference of measurement validity or multiple metrics. The measurement validity issue stems from differences in the accuracy and precision of eye-tracking equipment \citep{holmqvist2015a}, which can affect the data quality and bias effect sizes \citep{orquin2016a}. We developed a correction method that relies on an empirical estimate of the relationship between eye-tracker characteristics and observed effect sizes (see \textit{Methods}; Figure~\ref{fig:ET_accuracy_effectsize}; Table~\ref{tab:eyetracker_specifications}). Most metrics are based on fixations -- defined as maintaining the gaze on a single location or area of interest (AOI), such as fixation count, fixation likelihood, total dwell time and so on. This leads to a potential issue with comparing effect sizes reported with different dependent variables. We developed a correction method that makes the dependent variables comparable, where we empirically estimate correction factors based on a subset of studies in our sample that report multiple dependent variables (see \textit{Methods}; Figure~\ref{fig:metric_correction}; Table~\ref{tab:metric_correction}). This method allowed us to transform all effect sizes to a single metric; we decided for fixation count which was used in all meta-analyses. 

In what follows, we first analyse the group of visual factors and then the group of cognitive factors. We perform meta-analysis on each individual factor  separately. We next perform a small moderator analysis and finish with an analysis of publication bias in all the meta-analyses.  


\subsection{Almost all visual factors have substantial effect sizes}

We focused on four major groups of visual factors -- salience, position, surface size and set size (see \textit{Methods} for coding procedure). The summary effects of the visual factors on attention during decision making show that the factors range from small to medium effect sizes (see Table~\ref{tab:main_results} and Figure~\ref{fig:forest_plots_visual}). The trim and fill correction lead to adjustments only for left vs right position and set size. The left vs right position effect is corrected to a near null effect, but the small sample size, $k = 5$, casts some doubt on the precision of this correction. While the left vs right position does not seem to have an effect on fixation count we cannot exclude effects on temporal dynamics. For instance, decision makers may be more likely to begin with the left option and then move right \citep{fiedler2012} which could lead to primacy effects on choice. Overall, the trim and fill correction suggests that there is little publication bias in the visual factors. Inspecting the effect sizes, it is interesting that salience, which so far has taken the center stage in vision science, has only a small summary effect. The largest effects are those of center position, $r=.43$, and surface size, $r=.35$, which are larger than the trim and fill corrected cognitive factors which range between $r = .26-.41$. While it is possible to eliminate the effects of, for instance, center position and surface size in laboratory studies, this is not true for natural environments. In natural environments it is reasonable to expect that multiple, and most likely all, visual factors influence eye movements at the same time. Not only does the effects of the largest visual factors exceed the largest cognitive factors when comparing one by one, but considering their joint effect we believe it is reasonable to conclude that visual factors play a larger role than cognitive factors in determining visual attention in decision making. 

\input{tables/main_results.tex}

\begin{figure}[!h]
\includegraphics{forest_plots_visual}
\centering
\caption{Effect sizes of the visual factors are moderate, except for salience and left-vs-right position, which have small effect sizes, if any. Forest plots show the unattenuated effect size correlations for each study in a group, as well as average effect across the group. Forest plot in (A) shows the effect sizes for salience factor, in (B) for center position, in (C) for left vs right position, in  (D) for surface size, and in (E) for set size factor. Error bars represent the 95\% confidence interval around the mean.}
\label{fig:forest_plots_visual}
\end{figure}


\subsection{Cognitive factors have effect sizes similar in magnitude to visual factors}

Previous research has identified a wide range of cognitive factors that influence attention, such as goals, task instructions, and preferences \citep[for a review see][]{orquin2013a}. Here, we divided cognitive control factors into three groups: task instruction, preferential viewing and choice-gaze effect. In studies on task instructions, participants receive instructions concerning a specific decision goal, and with that, what is relevant to gaze at. For instance, the participants may be instructed on the validity of stimulus attributes \citep{krefeld-schwalb2019a}, or infer the level of validity themselves \citep{bialkova2014a}. In preferential viewing studies, the relevance should be equal to the subjective preferences. For example, some alternatives have higher subjective values than others \citep{kim2012a}. Because of this qualitative difference between the two domains, we treated studies on task instructions and preferential viewing separately. 
Inspecting the effect sizes reveals that the summary effects in the two types of studies are moderate and similar in magnitude (see Table~\ref{tab:main_results} and Figure~\ref{fig:forest_plots_cognitive}). Using a Wald test, we find that effect sizes of task instructions and preferential viewing are unlikely to differ, \input{./tables/difftest_task_pref.tex}. 
The test suggests that it makes no difference to eye movements whether the relevance of information is defined according to an externally specified goal or according to preferences. Note however, that the two effect sizes differ more when comparing the trim and fill adjusted effect sizes.

Choice-gaze effect refers to an effect in attention whereby decision makers spend more time gazing at the eventually chosen alternative. This effect, originally introduced by Shimojo and colleagues \citep{shimojo2003a} as a ``gaze-cascade'' effect, is well-established in the literature, prompting us to study it as a separate factor. This factor consists of studies reporting the difference in eye movements between the chosen alternative and all other (not chosen) alternatives. We find that choice-gaze effect has a large effect on eye movements which however is reduced to a medium effect, $r=.41$, after the trim and fill adjustment.


\begin{figure}[!h]
\includegraphics{forest_plots_cognitive}
\centering
\caption{Effect sizes of the three cognitive factors are moderate to large. Forest plots show the unattenuated effect size correlations for each study in a group, as well as average effect across the group. Forest plot in (A) shows the effect sizes for task instructions factor, in (B) for preferential viewing, and in (C) for the choice-gaze effect factor. Error bars represent the 95\% confidence interval around the mean.}
\label{fig:forest_plots_cognitive}
\end{figure}


\subsection{Alternative vs attribute moderator is significant only for the set size factor}

Alternatives that participants in judgment and decision making studies choose between can often be decomposed into constituent elements, commonly called attributes, cues or features \citep{payne1988,tversky1972elimination,stojic2020s,gigerenzer1996reasoning,schulz2018putting,hogarth2007heuristic}. For example, in classical lottery tasks \citep{tversky1979}, the probabilities and values of an alternative can be viewed as attributes. Or, in multi-cue judgment tasks, alternatives are more explicitly composed of cues -- university, major football team or main city in the German city size task \citep{gigerenzer1996reasoning}. This has consequences for both modelling of decision processes and units of analysis. Consequently, some studies in our sample focused on attention effects at either alternative or attribute level, or both. This was in particular the case for studies involving set size, task instructions, and preferential viewing factors. Since the alternative vs attribute dimension might be an important moderator in these groups, we decomposed them further with regards to the effect of alternatives vs attributes (Table~\ref{tab:mod_results} and Figure~\ref{fig:forest_plots_altatt}). Moderator analyses shows support for the alternative vs attribute moderator across set size, \input{./tables/moderator_setsize.tex}\unskip, but no support for preferential viewing, \input{./tables/moderator_pref.tex}\unskip, or for task instructions, \input{./tables/moderator_task.tex}\unskip. It is worth noting that effect sizes are consistently larger when operationalized at the level of alternatives compared to attributes (Table~\ref{tab:mod_results} and Figure~\ref{fig:forest_plots_altatt}). 

We also performed a moderator analysis for the choice-gaze effect factor, to assess whether the effect is driven by preferential viewing as proposed by \cite{shimojo2003a}. We compare studies with preferential vs inferential choice tasks and find no support for moderation by decision type, \input{./tables/moderator_choicebias.tex}\unskip, and therefore only report results for the main group. 


% -------------------------------------------------------
% Publication bias
% -------------------------------------------------------

\subsection{Publication bias exists, but its relatively small}

We assessed potential publication bias by first performing a precision-effect test \citep[PET][]{stanley2014}, which regresses individual study effect sizes on study standard deviations weighted by the study precision, controlling for the eye-tracker accuracy in addition (see \textit{Methods} for details). It is not recommended to use the test in case of small samples, e.g. less than 10 studies \citep{vanaert2019}, and we therefore used it on the complete data set. The PET shows a significant effect of standard deviation of the effect size, \input{tables/PETintext}\unskip, suggesting the presence of publication bias (see \textit{Appendix} Table \ref{tab:PET-PEESE} for full regression results). Given the significant PET, we then performed the precision-effect estimate with standard errors test (PEESE). The PEESE differs only in using the study variance instead of the standard deviation. The intercept in the PEESE is normally used as the publication bias corrected estimate, \input{tables/PEESEintext} (see \textit{Appendix} Table \ref{tab:PET-PEESE} for full regression results). The bias corrected estimate in the PEESE test is, in our case, not very informative by itself since it groups all independent variables into one estimate. However, by comparing the PEESE intercept to that obtained from an intercept only regression, \input{tables/FE}\unskip, we can get an impression of the degree of effect size overestimation due to publication bias. This comparison suggests that publication bias leads to an overestimation factor of \input{tables/peeseFactor}\unskip. 

We performed an additional analysis of publication bias, because of the above mentioned limitations of the PET-PEESE test. It has been shown that studies which receive public grants are more likely to be published \citep{canestaro2017}, and we therefore expected that acknowledging a public grant in the article may be associated with smaller effect sizes. We used a random effects model with public grant as moderator (see \textit{Methods}) and find that public grants have a marginally significant effect on effect sizes in the expected direction, \input{tables/publicSig}\unskip. Comparing public vs non-public funded research we derived an overestimation factor of \input{tables/publicFactor}\unskip. 

The PET-PEESE and public grant analyses are based on very different premises, but they both suggest the presence of publication bias. We therefore proceeded with a publication bias correction using the trim and fill analysis \citep{duval2000trim} at the level of each independent variable (see \textit{Methods}). The trim and fill analysis resulted in a downward adjustment of the average effect size for most of the independent variables. The corrected effect sizes in Table~\ref{tab:main_results} (in parentheses) provide a more conservative estimate of the true population effects, but are also subject to some uncertainty. Specifically, the interpretation of the corrected results may be biased due to heterogeneity in many of the factors as well as a relatively small number of studies in the visual factor groups. Comparing the uncorrected with the trim and fill corrected estimates suggests and overestimation factor due to publication bias of \input{tables/trimFactor}, which is within the range of the overestimation factors from the PET-PEESE and the public grant analysis. \\

The three analyses suggest an overestimation factor somewhere in the range of \input{tables/peeseFactor} to \input{tables/publicFactor}\unskip. While any factor above 1 is undesirable, it is worth noting it is far below the factor of 2.59 suggested by \cite{kvarven2020} who compared 15 pairs of meta-analyses estimates with estimates from many-lab replication studies. In addition to these analyses, we plotted the Fisher transformed correlation coefficients of each study by its respective standard error (so-called funnel plots; Figure~\ref{fig:funnel_plots} for main results, and Figure~\ref{fig:funnel_plots_altatt} for moderator analyses). The symmetry of the funnel plots provides a qualitative picture of publication bias since we expect that studies with smaller sample sizes and hence higher standard errors yield more variable effect sizes, the smallest of which are less likely to be published, leading to an asymmetric funnel plot. Funnel plots are generally rather difficult to interpret, and in our case it is almost impossible to draw any conclusion for visual factors, given smaller number of studies they are based on.


% -------------------------------------------------------
% Descriptive EM analysis
% -------------------------------------------------------


\subsection{Projecting effect sizes to original measurement units reveal substantial effects in absolute sense}

To better understand the results of the meta-analysis we extract for each effect size the corresponding descriptive eye movement data whenever the included study reports this information (see \textit{Methods}). For example, a study would report a fixation count (or likelihood or dwell time) for high vs low salience conditions. We extracted the descriptive eye movement data at the unit of a single AOI, e.g. corresponding to the fixation count for a single attribute level or for a single alternative if the AOIs are defined at this level. A total of \input{tables/authorEMcount} articles report descriptive eye movement data, resulting in \input{tables/EMcount} corresponding pairs of effect sizes and descriptive eye movement data, with some studies reporting results for more than one dependent variable. In total there are \input{tables/flEMcount} studies reporting fixation likelihood, \input{tables/fcEMcount} reporting fixation count, and \input{tables/tdtEMcount} reporting total dwell time.


\begin{figure}[!h]
\includegraphics{figs/EMtoES.pdf}
\centering
\caption{Descriptive eye movement data provides insight on the attention behaviour of an average participant and how effect sizes translate into the original measures they were based on. Distribution plots illustrate the fixation likelihood (A), the average fixation count (B), and the average total dwell time (C) across studies. We illustrate the linear relationship between the effect size correlation and the descriptive eye movement data by logit transforming differences in fixation likelihood (D), log transforming fixation count (E), and log transforming total dwell time (F). Lines are based on intercepts and slopes from linear mixed model fits (see main text and \textit{Methods} for details).}
\label{fig:em_figure}
\end{figure}


From averaged eye movement measures we see that the average for fixation likelihood is \input{tables/flmean}\unskip, for fixation count \input{tables/fcmean}\unskip, and total dwell time \input{tables/tdtmean}\unskip. From the distribution plots it is clear that there is a lot of variance in fixation likelihood across studies (Figure~\ref{fig:em_figure}). In some studies participants fixate nearly all AOIs, but there is also a large number of studies in which participants fixate half or less of the AOIs. The main use of the descriptive eye movement data, however, was to provide intuitions about the synthesized effect sizes. To this end, we transformed the synthesized effect size for each independent variable into its corresponding effect on fixation likelihood, fixation count and total dwell time (see \textit{Methods} for details). We achieved this transformation by regressing descriptive measures (appropriately transformed) on effect size correlations, using a linear mixed model with a random intercept, grouped by article to account for correlated errors. For fixation likelihood (logit difference between conditions) the model intercept is not significantly different from zero, whereas the slope is, \input{tables/FLtoLogitModel}\unskip. Figure \ref{fig:em_figure} panel (D) illustrates the relationship between the transformed statistic and effect size correlations, with an increasing variance as effect sizes become larger. For fixation count (log difference between conditions) the model shows a similar pattern where the intercept is not significantly different from zero, while the slope is, \input{tables/FCtoLogModel}\unskip, as illustrated in Figure~\ref{fig:em_figure} panel (E). The model for total dwell time (log difference between conditions) differs in having a significant intercept alongside a significant slope, \input{tables/TDTtoLogModel}\unskip, as illustrated in Figure~\ref{fig:em_figure} panel (F). Overall, all three measures strongly correlate with the (transformed) effect sizes, giving us confidence for converting effect sizes into original measures using the fitted models. More specifically, for each independent variable we computed the expected increase in descriptive eye movement measures based on the effect size, for a study with an average descriptive measure (see \textit{Methods} for details). Combining the estimates from these operations we could finally compare the effect sizes for each independent variable in terms of the equivalent effect on fixation likelihood, fixation count, and total dwell time for an average study (see Table~\ref{tab:em_results}). The table reveals surprisingly large effects on eye movements when expressing effect size correlations in raw eye movement metrics. Even the smallest effect observed for salience, $\rho = .13$, leads to an increase in fixation likelihood from an average of $61\%$ to $76\%$. Using a central location increases fixation likelihood to $91\%$. For fixation count the numbers are equally surprising. From an average fixation count of $7.8$ increasing salience can add three more fixations to a piece of information while using a central location can add nearly ten more fixations. For the purpose of policy interventions aiming to capture decision maker attention these numbers are beyond all our expectations.

\input{tables/em_results}
