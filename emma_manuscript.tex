%-*- program: pdflatex -*-
%-*- program: bibtex -*-
%-*- program: pdflatex -*-
%-*- program: pdflatex -*-


% --------------------------------------------------------------
% Preamble
% --------------------------------------------------------------

\documentclass{article}
\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\usepackage{booktabs}       % professional-quality tables
\usepackage{tabularx}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{gensymb}        % degree, angle symbols

% for math equations and symbols
\usepackage{amsmath} 
\usepackage{amssymb} 
\newcommand{\E}{\mathbb{E}}
\newcommand{\SD}{\mathit{SD}}
\newcommand{\SE}{\mathit{SE}}
\newcommand{\BF}{\mathit{BF}}
\DeclareMathOperator\arctanh{arctanh}

% This prevents placing floats before a section.
\usepackage{placeins}

% bibliography
\usepackage{natbib}


%%% some support for commenting

\usepackage{color}
\definecolor{Blue}{RGB}{0,0,255}
\definecolor{Red}{RGB}{255,0,0}
\newcommand{\jo}[1]{\textcolor{Red}{[Jacob: #1]}}  
\newcommand{\hs}[1]{\textcolor{Blue}{[Hrvoje: #1]}} 
% \newcommand{\jo}[1]{}  
% \newcommand{\hs}[1]{} 


% ---------------------------------------------------------
% Title, authors
% ---------------------------------------------------------

\title{A meta-analysis of eye movements in decision-making}

% Alternatives
% The visual environment influences attention during decision making
% Visual environments, attention and decision making
% A meta analysis of influence of the visual environement factors on  attention in decision making

\author{
  Jacob L. Orquin\thanks{Correspondence concerning this article should be addressed to Jacob L. Orquin, Department of Management/MAPP, Aarhus University, Fuglesangs alle 4, 8210 Aarhus V - Denmark. E-mail: jalo@mgmt.au.dk. Data and scripts are available at: \url{https://osf.io/buk7p/}. This research was supported by the Independent Research Fund Denmark, grant number: 8046-00014A. The authors thank Martin Meissner, Tobias Otterbring, Sonja Perkovic, and Valdimar Sigurdsson for feedback on earlier versions of the manuscript.}, Erik S. Lahm, Hrvoje Stojić
}


\begin{document}
\maketitle

% ---------------------------------------------------------
% Abstract
% ---------------------------------------------------------


\begin{abstract}  % NHB limit: 150 words
As decision-makers, we rely on our eyes for gathering information in a vast number of tasks every day. A growing body of research attempts to understand what drives eye movements in decision-making and how eye movements are related to the decision process. Different models have been proposed, assuming that eye movements are driven separately or in combination by either by goal related top-down factors or stimulus related bottom-up factors. We meta-analyze empirical studies on eye movements in decision-making and provide a quantitative overview of some of the factors driving eye movements. Our findings reveal that the bottom-up factors, salience ($\rho = .13$), surface size ($\rho = .38$), left vs right positioning ($\rho = .27$), central positioning ($\rho = .43$), and set size ($\rho = .28$), all influence what decision-makers gaze at independently of the decision task or preferences. Top-down factors have a slightly larger effect on eye movements than bottom-up factors: decision-makers gaze more at information that is relevant in terms of task goals ($\rho = .42$) or subjective preferences ($\rho = .48$) and gaze more at the eventually chosen option ($\rho = .70$) regardless of whether the choice is based on preferences or inferences. Our findings show a need for incorporating bottom-up factors in models that jointly explain eye movements and decision-making.
\end{abstract}

\keywords{eye movements, decision-making, meta-analysis, top-down control, bottom-up control} 


% -------------------------------------------------------
% Introduction
% -------------------------------------------------------

\section{Significance statement}

In decision making we often gather information with our eyes. We show that decision makers are more likely to gaze at information that is relevant to the decision task or their preferences, but also gaze more at information when it is salient, large, centrally positioned, and uncluttered independently of its relevance. Because the structure of our visual environment plays a large role in determining eye movements, citizens and policy makers should be aware of who is in control of the visual environment and how it can bias information processing in favor of or against the interests of decision makers.   


\section{Introduction}

% \section{Motivation and Problem}

Decision making often takes place in environments where relevant information needs to be acquired visually. In such visual environments options can differ in their position, surface size, salience and many other visual properties. Consider encountering a product with surprising color on a  supermarket shelf, or a restaurant menu where items either have or do not have an accompanying picture. Such visual properties have been shown to influence our attention during decision making in real world situations and there is now substantial evidence showing that attention plays an important role in decision making \citep{gidloef2017a}, and can even causally affect choices \citep{ghaffari2018a, paernamets2015a, shimojo2003a}. However, the role of visual properties is almost completely absent from prominent decision making theories. In most theories, the decision task determines the relevance of objects and, either explicitly or implicitly, whether and when we look at them. Here, we ask whether decision research is building on correct assumptions about visual attention and the role of the visual environment and provide an empirical assay of the relative importance of various visual and task factors to guide further theory development.


% \section{Decision research (mostly) ignores bottom-up factors}

Most decision research considers attention to be determined by the decision process, that it is driven by the goal relevance of objects rather than the visual properties of these objects. In many prominent decision making models this assumption is implicit. Consider, for example, the prospect theory model of how probabilities and values of options are integrated  to arrive at a preferential choice \citep{tversky1979}. Options are treated equally according to this model, and nothing in the model indicates that one piece of information would attract more attention than the other. Prospect theory and other related variants of expected utility theory are focused on capturing the final choices, not the process of how people arrive at choices, which one could argue is more important for attention. However, popular process-oriented decision making models commit to similar assumptions about attention. Consider, for example, satisficing, elimination-by-aspect, or the lexicographic heuristics \citep{payne1988, simon1956a}. While these models all specify different information search processes, they make similar implicit assumptions about the nature of visual search and hence attention in decision making. These  models assume that the information search is determined by a search rule inherent to the decision process, e.g. attend to options one at a time until a satisfactory option is found \citep{stuttgen2012}, or attend to information cues in order of their validity until a cue is found that identifies the best option \citep{krefeld-schwalb2019a}. 

In sequential sampling models of decision making attention has had a more explicit role. Sequential sampling models assume that stochastic evidence for an alternative is accumulated over time and when the integrated evidence reaches a threshold a choice is made. This is a process-oriented model that aims to capture how people balance the value of accumulating more information with the cost of taking more time to reach a decision \citep{forstmann2016}. In two influential variants of these models attention plays an important role, by determining how evidence is sampled in favor of choice options \citep{busemeyer1992} or by determining the weight assigned to the evidence \citep{krajbich2010a}. In these models attention, fluctuates randomly between choice options or choice attributes until a choice is made. The implicit assumption being, that in the long run attention is uniformly distributed over options and attributes. This is a stochastic equivalent to a maximizing decision rule such as the weighted additive which assumes that a decision maker attends equally to all information \cite{gloeckner2011a, payne1988}. In other words, even though attention exerts influence on choices, this influence is random and neither controlled by goals or the visual environment. Recently, sequential sampling models have been proposed in which attention is guided by the value of choice options \citep{callaway2019a, gluth2018, gluth2020, krajbich2010a}. This assumption is supported by empirical findings demonstrating value based attentional capture, i.e. the effect that objects associated with rewards capture attention \citep{lepelley2015}. The models are reminiscent of an earlier idea by \cite{shimojo2003a} who proposed that decision makers attend preferentially to high value options, which increases their value further, thus creating a feedback loop and increasing likelihood of gazing at the ultimately chosen option. 

There are a few studies that proposed decision making models where attention is not driven only by the goal relevance of options, but also by their visual properties, focusing on visual salience. For example, \cite{towal2013a} showed that visual salience continuously influences the decision process by making some choice options more likely to attract fixations, but does not influence the drift rate towards salient choice options directly. \cite{chen2013} provided evidence that salience can determine the onset of drift towards a choice option, but not the drift rate itself. Finally, \cite{navalpakkam2010} showed that decision makers in a reward harvesting task made choices by combining value and visual salience, consistent with an ideal Bayesian observer. This work suggests that salience can influence the decision process directly rather than by biasing attention, and, through that, the onset or amount of drift. 


% \section{Why do we need bottom up factors in decision making models?}

The assumption in decision science about cognitive factors being the only or main factor driving attention in decision making is inconsistent with a number of findings. \cite{vanderlans2008}, for instance, find that 2/3 of variance in attention is due to factors in the visual environment, unrelated to the decision task, and \cite{towal2013a} and colleagues find that 1/3 of variance is due to stimulus factors. There are also several model free studies showing comparative effects of cognitive and visual factors on attention in decision making \citep{gidloef2017a, orquin2015a, orquin2019a}. Moreover, there is evidence that the visual environment influences choices by biasing visual attention. For instance, decision irrelevant visual factors have been shown to influence choices by changing the amount of gaze \citep{peschel2019,chandon2009a} or the order of gaze \citep{reeck2017a}. Even studies examining purely cognitive models of decision making often implicitly acknowledge the influence of visual factors by taking great effort to eliminate them by controlling the size, position, and salience of information \citep{brandstatter2014, gloeckner2011a, perkovic2018}.

Further evidence for the role of visual factors comes from vision science research. The few studies that modelled the influence of the visual environment on attention in decision making focused exclusively on salience \citep{chen2013,navalpakkam2010, towal2013a}. This focus seems justified - a great deal of research in vision science has concentrated on visual salience (for a review see \cite{borji2012a}). The term visual salience refers to the stimuli that differ from their surroundings in terms of visual conspicuity and it has been shown that observers are more likely to gaze at stimuli that are high in visual salience \cite{itti2000}. However, there has been a much debate about the role of salience in guiding attention some arguing that it plays no role in, for instance, real world behavior \citep{tatler2011a}. Besides salience, there are at least three other visual factors that are likely to guide attention in decision making. 
One factor is the relative surface size of stimuli, which refers to the proportion of the visual environment occupied by the stimulus \citep[for a review see][]{peschel2013a}. Increasing the surface size of choice options has been shown to increase gaze by up to 25 \% \citep{chandon2009a} Increments to surface size exhibit a diminishing marginal effect on eye movements \citep{lohse1997a}. A second factor is the position of stimuli which has been shown to influence eye movements and is sometimes corrected for in vision research models when estimating the influence of other variables of interest \citep{clarke2014a}. In decision making context options are normally placed in different spatial locations, which means that position effects like left-to-right (reading) direction and centrality are likely to influence eye movements and choices \citep{atalay2012a, meissner2016a}. A third factor is the number of stimuli, the set size, which in decision context normally is operationalized as the number of alternatives or attributes. Increasing the set size generally slows reaction times to identify search targets \citep{wolfe2010} and may also increase the visual complexity by the addition of more and different visual stimuli. Visual complexity has been shown to increase the difficulty and amount of visual search \citep{rosenholtz2007a}, but also the amount of attention consumers pay to print advertisement \citep{pieters2010a}. 

An important point about these visual factors is that all four are likely to vary in natural environments and have been shown to affect attention simultaneously \citep{orquin2019a}. While decision research often sees the visual environment as a nuisance factor and try to eliminate its influence on decision making \citep{brandstatter2014, gloeckner2011a, perkovic2018}, companies and governments often use the same factors to compete for the attention of consumers and citizens \citep{pieters2017, orquinwedel2020}.   

Despite these findings on the importance of visual factors in attention and decision making, we have seen only a small impact on theory development. While attention and cognitive influences hereupon recently started playing a prominent role in decision making theories \citep{callaway2019a, gluth2018, gluth2020, krajbich2010a}, the role of visual factors has been largely ignored. There are only a few studies that have proposed and tested models that incorporate the influence of the visual environment on attention in decision making \citep{chen2013, navalpakkam2010, towal2013a}. Moreover, these studies have focused exclusively on salience, despite the many other visual factors that are likely to be relevant as well and their joint contribution. 

A systematic review that provide evidence on how important visual factors are individually, as well as relative to cognitive factors, would give a new impetus to research and theory development incorporating the role of the visual environment; or justify the lack of it. The increasing availability of eye-tracking equipment has paved the way for such a review. Eye-tracking provides a way to unobtrusively measure the influence of both visual and cognitive factors on attention in decision making tasks. In the last two decades numerous model free eye-tracking studies appeared, situated in a decision making setting. These studies span many disciplines, from vision science, neuroscience and cognitive psychology to behavioural economics and consumer psychology, which potentially explains why such a review has not been done before.


% \section{Study approach} 

Here, we assess the importance of the visual environment in decision making by empirically examining the magnitude of effects of various visual factors on attention in decision making and comparing them with cognitive factors. We focus on four visual factors – salience, relative position, surface size and choice set size – and three cognitive factors – task effects, preferential viewing and choice bias. We collect effect sizes from empirical studies on eye movements in decision making and meta-analyze them to get reliable effect estimates. Our findings show that among the visual factors position in the centre of the field of view has the largest effect, while visual salience has the smallest effect on attention. Visual factors have slightly smaller effects on eye movements than cognitive factors.  However, since all visual factors can influence attention simultaneously, in cases with multiple factors, these could jointly have a larger influence than cognitive factors. Overall, these results show that characteristics of the visual environment has a large and consistent effects on eye movements in decision making and that the effects are present across various decision contexts and tasks. This suggests that future theories and models of decision making should integrate visual factors directly rather than see them as nuisance factors.


% -------------------------------------------------------
% Results
% -------------------------------------------------------

\section{Results}

Our initial literature search retrieved 1981 article, of which 454 remained after screening of the title and abstract. Following a more detailed evaluation whether studies were on decision making and used eye tracking, we identified 291 article as potentially eligible studies. Based on detailed inspection of their full texts, 58 articles satisfied all inclusion criteria and were included in the meta-analysis. Figure~\ref{fig:flow_diagram} illustrates the PRISMA flow diagram \citep{moher2009preferred}. Many of these articles consist of multiple experiments and each experiment was counted as a separate study. This resulted in 106 studies, out of which 39 studied the effects of visual factors and 67 studied the effects of cognitive factors.


\begin{figure}[H]
\includegraphics[width=0.8\textwidth]{prisma}
\centering
\caption{The PRISMA flow diagram showing the results of the literature search.}
\label{fig:flow_diagram}
\end{figure}


Meta-analyses on eye movements are rare and there are several methodological issues. There are two main issues we had to resolve before performing the meta-analyses: handling measurement validity across different eye tracker types and comparing different eye movement dependent variables. To handle these issues, we use a psychometric meta-analysis \citep{hunter2004a}, which allows us to quantify the interference of measurement validity or multiple metrics. The measurement validity issue stems from differences in accuracy and precision of eye tracking equipment \citep{holmqvist2015a}, which can affect the data quality and bias effect sizes \citep{orquin2016a}. We developed a correction method that relies on empirical estimate of relationship between eye tracker characteristics and observed effect sizes (see \textit{Methods}; Figure~\ref{fig:ET_accuracy_effectsize}; Table~\ref{tab:eyetracker_specifications}). There are multiple eye movement metrics commonly used in studies, all based on fixation -- defined as maintaining the gaze on a single location or area of interest (AOI), such as fixation count, fixation likelihood, total fixation duration and so on. This leads to a potential issue with comparing effect sizes reported in different metrics. We developed a correction method that makes all metrics comparable, where we empirically estimate correction factors based on a subset of studies in our sample that report multiple dependent variables (see \textit{Methods}; Figure~\ref{fig:metric_correction}; Table~\ref{tab:metric_correction}). % Results on all bottom-up factors are presented in fixation likelihood and results on top-down factors are presented in fixation count (effect sizes on both metrics are comparable, see Table~\ref{tab:metric_correction}).

In what follows, we first analyse the visual group of factors and then cognitive group. We perform meta-analysis on each individual factor  separately. We next perform a small moderator analysis and finish with the analysis of publication bias in all the meta-analyses.  


\subsection{Visual factors}

quick motivation for the visual factors...

The main results are presented in Table~\ref{tab:main_results}, with the most important variables being the corrected effect size estimate, $\rho$, and the associated heterogeneity, $I^2$. Except for salience, $\rho = .15$, the results show that all visual factors have medium to large effect sizes ranging from $\rho = .42$ to $\rho = .57$. (Table~\ref{tab:main_results} and Figure~\ref{fig:forest_plots_visual})

% Considering that there is no bottom-up free visual environment, it is reasonable to expect that all bottom-up factors influence eye movements at the same time. In laboratory environments, it is possible, and often desirable, to control for bottom-up factors, but in natural environments where no such control or counterbalancing takes place, the visual environment could account for a large share of variance in eye movements. The mounting evidence that demonstrates a causal effect of eye movements on decision-making (Ghaffari \& Fiedler, 2018; Reeck et al., 2017; Pärnamets et al., 2015; Shimojo et al., 2003), therefore, suggest an important role for the visual environment in guiding our decisions through our eye movements. 


% \caption{Main results of the meta-analysis divided into independent variable subgroups }
% \label{tab:main_results}
\input{tables/main_results.tex}


\begin{figure}%[H]
\includegraphics{forest_plots_visual}
\centering
\caption{Effect sizes of all four visual factors are sizeable but not large. Forest plots show the unattenuated effect size correlations for each study in a group, as well as average effect across the group. Forest plot in (A) shows the effect sizes for salience factor, in (B) for left vs right position factor, in  (C) for central position factor, and in (D) for set size factor. Error bars represent the 95\% confidence interval of the mean.}
\label{fig:forest_plots_visual}
\end{figure}


\subsection{Cognitive factors}

We divided cognitive control factors into three subgroups: inferential viewing, preferential viewing and choice bias. In studies on inferential viewing, participants receive instructions concerning a specific decision goal, whereas, in preferential viewing studies, participants decide based on subjective preferences. Because of this qualitative difference between the two domains, we treated studies on inferential viewing and preferential viewing separately. The inspection of the effect sizes reveals that the main effect in the two types of studies are practically indistinguishable. In inferential viewing $\rho = .495$ and in preferential viewing $\rho = .511$, with overlapping credibility and confidence intervals (Table~\ref{tab:main_results} and Figure~\ref{fig:forest_plots_cognitive}). This result suggests that it makes no difference to eye movements whether the relevance of information is defined according to an externally specified goal or according to preferences. 

We study the third subgroup, choice bias, separately because of a well-established relation between chosen option and attention, popularly called ``gaze-cascade'' effect \citep{shimojo2003a}. These studies reported the difference in eye movements between the chosen option and all other (not chosen) options (there are several studies that are in previous two subgroups as well). We find that choice bias has a very large effect on eye movements, $\rho = .77$ (Table~\ref{tab:main_results} and Figure~\ref{fig:forest_plots_cognitive}). %The choice bias effect is similar for preferential and inferential decision tasks, suggesting that eye movements are not driven by preferential viewing. Even in tasks where the chosen option is not the preferred option, decision-makers make more fixations to it.

In comparison to visual factors, all three cognitive factors are large, ranging from $\rho = .50$to $\rho = .77$, but not three times as large as previously hypothesized \citep{wedel2006chapter}. However, it should be noted that there is no (natural) environment free of visual factors, and it is reasonable to expect that multiple visual factors influence eye movements at the same time. Depending on the presence of visual factors in a particular environment, visual factors have the potential to be a major driver of the attention in a choice setting.


\begin{figure}%[H]
\includegraphics{forest_plots_cognitive}
\centering
\caption{Effect sizes of all three cognitive factors are large. Forest plots show the unattenuated effect size correlations for each study in a group, as well as average effect across the group. Forest plot in (A) shows the effect sizes for inferential viewing factor, in (B) for preferential viewing, and in (C) for the choice bias factor. Error bars represent the 95\% confidence interval of the mean.}
\label{fig:forest_plots_cognitive}
\end{figure}


\subsection{Moderator analyses}

We also perform moderator analyses of set size, inferential viewing, and preferential viewing with regards to the effect of alternatives vs attributes.

Motivate this...

We find weak support for the alternative vs attribute moderator across set size, \input{./tables/moderator_setsize.tex}, inferential viewing, \input{./tables/moderator_task.tex}, and preferential viewing, \input{./tables/moderator_pref.tex}. While none of the moderators are significant at .05, it is noteworthy that effect sizes are consistently larger when operationalized at the level of alternatives compared to attributes (Table~\ref{tab:main_results}). In the following we report results for the main and moderator groups to allow readers to form their own opinion. 

Breaking down both groups by moderators reveal further similarities. Both preferential viewing and inferential viewing are moderated by the subgroups alternatives and attribute, and in both cases, there is a larger effect at the alternative level. These findings dovetail with the moderation of set size where we also find a larger effect on set size by alternatives than set size by attributes. The result implies that decision-makers are more likely to adapt to increases in set size by ignoring alternatives than by ignoring attributes. One interpretation is that decision-makers are more oriented towards compensatory than non-compensatory decision rules, e.g., more likely to use satisficing than the lexicographic decision rules. 


and a moderator analysis of choice bias with regards to the effect of inferential vs preferential tasks.

Motivate this...

The choice bias effect is similar for preferential and inferential decision tasks, suggesting that eye movements are not driven by preferential viewing. Even in tasks where the chosen option is not the preferred option, decision-makers make more fixations to it.

We find no support for moderation of choice bias by inferential vs preferential task, \input{./tables/moderator_choicebias.tex}, and only report results for the main group. 

Finally, we test the probability that the synthesized effect sizes of inferential viewing, preferential viewing, and choice bias stem from different populations. Using a Wald test, we find that effect sizes of inferential viewing and preferential viewing are unlikely to differ, \input{./tables/difftest_task_pref.tex}, inferential viewing and choice bias are likely to differ, \input{./tables/difftest_task_choice.tex}, and preferential viewing and choice bias somewhat likely to differ, \input{./tables/difftest_pref_choice.tex}.


\subsection{Publication bias}

We assessed potential publication bias using a trim-and-fill analysis \citep{duval2000trim} of each subgroup. In addition, we plotted the  Fisher transformed correlation coefficients of each study by its respective standard error (so-called funnel plots; Figure~\ref{fig:funnel_plots} for main results, and Figure~\ref{fig:funnel_plots_altatt} for moderator analyses). The symmetry of the funnel plots provides a qualitative picture of whether there is a file drawer problem. We expect that studies with smaller sample sizes and hence higher standard errors yield more variable effect sizes, the smallest of which are less likely to be published, leading to an asymmetric funnel plot. The trim-and-fill analysis resulted in a downward adjustment of the average effect size for most of the subgroups. The corrected effect sizes in Table~\ref{tab:trim_fill_results} provide a more conservative estimate of the true population effects, but are also subject to some uncertainty. Specifically, the interpretation of the corrected results may be biased due to heterogeneity in many of the subgroups as well as a relatively small number of studies in the visual factor groups. 


\begin{figure}%[H]
\includegraphics{funnel_plots}
\centering
\caption{Funnel plots for each study subgroup that can be used as a qualitative check of a publication bias. Points are Fishers transformed correlation coefficients against their standard error. Asymmetric distributions of points can indicate the presence of publication bias since smaller studies (those with higher standard errors) have more variable effect sizes and are less likely to be published unless the effect is large. Funnel plot for (A) salience, (B) surface size, (C) left vs right position, (D) central position, (E) set size, (F) inferential viewing, (G) preferential viewing, and (H) choice bias.}
\label{fig:funnel_plots}
\end{figure}


% -------------------------------------------------------
% Discussion
% -------------------------------------------------------


\section{Discussion}

%%% Other ideas

% - choice bias confound
% - 

%%% 1. Brief summary
% - roughly the current opening paragraph? change according to our current framing
% - add main findings - 2-3 sentences\\

For the better part of our daily lives, we attend to and gather information using our eyes. We visually inspect food products in the supermarket or online, we read emails, task lists, memos, or software and spend considerable time looking at other people and our surroundings in an attempt to navigate our social and physical environment. Consequently, many of the decisions we make, small or large, are based on the visual information that we gather using our eyes. In this study, we attempt to answer how our eyes gather information during decision-making. We meta-analyze empirical studies on eye movements in decision-making and provide a quantitative overview of the factors driving eye movements in decision-making. We distinguish between internal or top-down factors such as preferential viewing, inferential viewing, and choice bias and external or bottom-up factors such as salience, surface size, set size, and position effects. We identify 107 effect sizes and perform a psychometric meta-analysis to control for issues related to the validity of each study. 


% \subsection{Main findings}
%%%  2. Implications for theory

% - this should be a major one
% - move beyond salience, how can we integrate all of them in a single model?\\
% - evaluating the models!\\
% - original three studies raised a few interesting questions already that still needs to be answered: For instance, does visual salience and other bottom up factors consistently bias attention in decision situations, or is the effect limited to decisions under time pressure as in the studies by the Navalpakkam, Chen, and Towal author teams? If salience mainly influences attention immediately after stimulus onset (REF, but see REF), the effect of salience on attention and choice may diminish as the decision time extends. In other words, is the effect of salience a consequence of artificially induced time pressure?\\
% - We also do not know whether the effect of bottom up factors is limited to attention, and if so whether they influence the onset or amount of drift, or whether bottom up features are integrated with value as suggested by Navalpakkam and colleagues.\\
% - Preferences vs inferences have equal effect sizes: can we infer smth based on this – probably a lot of speculations\\


Except for salience, $\rho = .15$, the results show that all bottom-up factors have medium to large effect sizes ranging from $\rho = .42$ to $\rho = .57$. In comparison, all three top-down factors are large, ranging from $\rho = .50$to $\rho = .77$, but not three times as large as previously hypothesized (Wedel \& Pieters, 2006). Considering that there is no bottom-up free visual environment, it is reasonable to expect that all bottom-up factors influence eye movements at the same time. In laboratory environments, it is possible, and often desirable, to control for bottom-up factors, but in natural environments where no such control or counterbalancing takes place, the visual environment could account for a large share of variance in eye movements. The mounting evidence that demonstrates a causal effect of eye movements on decision-making (Ghaffari \& Fiedler, 2018; Reeck et al., 2017; Pärnamets et al., 2015; Shimojo et al., 2003), therefore, suggest an important role for the visual environment in guiding our decisions through our eye movements. 

Regarding top-down factors, several findings have emerged. We decided to analyze studies on inferential viewing and preferential viewing separately since there is a clear qualitative difference between the two domains. In studies on inferential viewing, participants receive instructions concerning a specific decision goal, whereas, in preferential viewing studies, participants decide based on subjective preferences. The inspection of the effect sizes reveals that the main effect in the two types of studies are indistinguishable, i.e., similar effect sizes, $\rho$ task = .495 and $\rho$ preferential viewing  = .511 and overlapping credibility and confidence intervals. This result suggests that it makes no difference to eye movements whether the relevance of information is defined according to an externally specified goal or according to preferences. Breaking down both groups by moderators reveal further similarities. Both preferential viewing and inferential viewing are moderated by the subgroups alternatives and attribute, and in both cases, there is a larger effect at the alternative level. These findings dovetail with the moderation of set size where we also find a larger effect on set size by alternatives than set size by attributes. The result implies that decision-makers are more likely to adapt to increases in set size by ignoring alternatives than by ignoring attributes. One interpretation is that decision-makers are more oriented towards compensatory than non-compensatory decision rules, e.g., more likely to use satisficing than the lexicographic decision rules. 

Finally, we find that choice bias has a very large effect on eye movements. The choice bias effect is similar for preferential and inferential decision tasks, suggesting that eye movements are not driven by preferential viewing. Even in tasks where the chosen option is not the preferred option, decision-makers make more fixations to it. There could be several explanations for this finding. The choice bias might arise because of  a) the gaze cascade phenomenon (Shimojo et al., 2003), b) an evidence accumulation process as proposed to, for instance, the aDDM model (Krajbich, Armel, \& Rangel, 2010), c) coherence maximization as proposed in the parallel constraint satisfaction model (Glöckner \& Herbold, 2011), d) the result of a resource-rational process in which decision-makers prioritize attention (Callaway \& Griffiths, 2019), or e) a mere consequence of a preparations for a motor response towards the chosen option (Hayhoe \& Ballard, 2014). The specific mechanism behind choice bias remains unclear; but considering how large the effect is, and the number of models that imply this effect, we believe that a better, and eventually full understanding of the effect will help advance decision research. 


% \subsection{Implications for other disciplines}
%%% 3. Broad impact

% - roughly existing 2 paragraphs, but make more succinct and merge into 1?
% + vision, neuro, cog psych, beh econ, consumer psychology...\\

Our findings have implications for several scientific disciplines. We have focused our discussion on judgment and decision-making but will turn a few other topics. Our meta-analysis is, to the best of our knowledge, the first to quantitatively summarize findings on top-down and bottom-up control of eye movements in any discipline, including vision science. Naturally, other disciplines may want to take stock of these findings and have to evaluate the generalizability of the findings to their respective discipline. Since JDM is a broad term, several disciplines such as cognitive psychology, behavioral economics, and consumer psychology are represented in the set of included studies. For these disciplines, our findings provide a useful framework for developing choice architecture interventions (Münscher, Vetter, \& Scheuerle, 2016) based on bottom-up factors. Our findings also point to the possibility of measuring individual preferences in real time through eye movements – a technique that may become practically relevant once our devices have built-in eye trackers (Bulling \& Wedel, 2019).    

Given the high degree of variance in methodologies and stimuli, we expect that our results generalize well to other disciplines such as learning and education research, problem solving, or human-computer interaction. The disciplines studying eye movements in natural environments, e.g., driving, aviation, or other natural tasks, should be cautious when applying our findings since the vast majority of the included effect sizes were from laboratory-based studies. In general, our findings attest to an unexpectedly large influence of the visual environment on our eye movements. We believe that this pattern is likely to replicate in other disciplines as well.


% \subsection{Methodological contributions}
%%% 4. Methodological contributions

% - roughly existing paragraphs (plus from previous limitation section), but more succinct, we should have one, at most two paragraphs here
% - briefly, correction methods for methodological issues we have developed have some limitations (fisher transform, underlying metrics distributed differently etc) - but addressing them seriously would require a full methods paper...\\

So far, only a few meta-analyses have been published on eye movements, and no guidelines exist concerning how to deal with eye-tracking-specific issues in meta-analyses. To perform our analysis, we have developed procedures for how to handle issues related to multiple AOIs, multiple metrics, and eye tracker validity. The procedure for handling eye tracker validity showed that eye trackers with poorer accuracy, in general, lead to lower effect sizes. In our data, the difference in Ai between the best and worst eye trackers were spanning from .167 to .667. This result is a substantial difference. Accounting for eye tracker validity improved the precision of the synthesized effect sizes. This finding is, furthermore, an important methodological contribution which demonstrates the relevance of ensuring high-quality eye tracker data.

Regarding the multiple metrics such as fixation count, fixation likelihood, or dwell, count, we showed, first of all, that these metrics yield very similar effect sizes, and, second, that the effect sizes expressed in one metric can be converted to an effect size expressed in another metric. Regarding studies which report multiple AOI's, we propose to average the effect sizes across AOI's. An alternative would have been to weight the effect sizes depending on the relevance of the AOI to the research question, but this approach would have required precise and unambiguous predictions regarding the relevance of each AOI. Such precise predictions are rare but may become more frequent in the future since other researchers can now derive predictions based on our findings.   

From a methodological perspective, future research may further develop our framework for correcting for eye tracker accuracy. We know that several factors contribute to the validity of eye trackers, e.g., accuracy depends on the stimulus and the AOI size (Orquin \& Holmqvist, 2018) and other artifacts such as sample population and recording location also matter (Nyström, Andersson, \& Holmqvist, 2013). By extending our framework to include these other artifacts, it will be possible to make more precise estimates of effect sizes in meta-analysis and individual studies as well as more realistic power analyses.   


% \subsection{Limitations}
%%% 5. Limitations

% - first existing paragraph\\
% - few other possible topics:
% - can we really have model-free estimates of top-down influences?\\
% - meta analyses suffer from some flaws – concerns of publication bias and selective reporting\\
% - probably related to these flaws above, “meta-analytic effect sizes are almost three times as large as replication effect sizes” (Kvarven, stromland, johannesson, 2019)\\
% - in that case, modelling studies become even more pertinent to investigate the mechansism behind and verify these influences in controlled studies...\\

Several limitations deserve to be mentioned. Most importantly, several of the bottom-up factors included a low number of studies which casts some doubt about the precision of the results. The low number of studies also analyzed the publication bias less reliable, thereby, adding to the uncertainty. Another challenge is that the studies included varied substantially because of stimulus differences, e.g., high vs. low complexity stimuli, and because of task differences, e.g., risky gambles vs. consumer decision-making. These differences may have introduced additional heterogeneity into the synthesized effect sizes, but at the same time, serve to increase the generalizability of the findings (Cooper, Hedges, \& Valentine, 2009). 

Another important avenue will be to gain a better understanding of the phenomenon of choice bias. How does it arise and what does it imply about the attention-choice process?  


%%% 6. Finish with moving toward decision making in wild!

% - finishing on a more positive note, rather than limitations
% - real life decision situations do not follow the lab rules, all options of equal sizes etc\\
% - these developments will improve our understanding of decision making outside of the lab\\
% - add some parts from the current limitation section\\

Our findings call into question several assumptions about how decision-makers search for and gather information. Several existing theories and models make assumptions that imply only a subset of the identified top-down and bottom-up factors. While these models may work in a controlled laboratory environment, it is clear that they do not generalize to more natural environments. Future models should, therefore, strive to incorporate a wide set of the identified factors to balance between top-down and bottom-up control. 


% -----------------------------------------------------------
% Method
% -----------------------------------------------------------

\section{Methods}

\subsection{Literature search}

Web of Science was searched using the following terms: eye track* OR eye move* OR eye fix* AND decision-making OR choice. Grey literature, such as reports and unpublished work, was identified in the first 2,000 hits on Google Scholar. No restrictions on publication date or language were imposed. Additional literature was identified by searching the reference lists of the identified papers and through contact with the authors. Calls for unpublished studies were distributed to the relevant research communities via email lists during February 2018 at the following lists; European Association for Decision Making (EADM), Society for Judgment and Decision Making (SJDM), and European Group of Process Tracing Studies (EGPROC). The search resulted in 291 studies screened for eligibility. The last search was done on March 1st, 2018.


\subsection{Inclusion criteria}

We included studies in which participants made decisions or judgments between discrete alternatives while their eye movements were monitored using eye-tracking technology. We did not include studies related to perceptual judgments, such as categorizing or discriminating visual stimuli or studies on problem solving. We excluded studies where participants were selected based on clinical diagnosis or specific socio-demographic traits e.g., visual disorders, age-related visual diseases, age restrictions such as adolescents or infants. Studies using fixed exposure time or time pressure manipulations were excluded since these manipulations can influence eye movement processes \citep{orquin2018a} and lead to different results \citep{simola2019a}. Included studies used either fixation likelihood (are of interest (AOI) looked at or not), fixation count (number of fixations to AOI), total fixation duration (sum of durations of all fixation to an AOI), or dwell count (number of dwells to an AOI). Eventually, 58 articles met all inclusion criteria and were included in the meta-analysis (Figure~\ref{fig:flow_diagram}).


\subsection{Data extraction and coding procedure}

The included studies were coded with regards to their (1) effect size, (2) sample size, (3) research domain, (4) eye tracker model, (5) dependent variable, and (6) independent variable. All studies were initially coded by the first author and later by the second author. Any disagreement was resolved by discussion. Agreement for categorical variables was assessed using Cohen's kappa and for continuous variables using intraclass correlation coefficient \citep{shrout1979a}. Overall, there was a high level of agreement: \input{./tables/intercoder_reliability.tex}.%
%
\footnote{Most of the printed coding sheets were unfortunately lost while moving between offices. The inter coder reliability is therefore computed on 20 observations from the random coding sheets that were recovered.}

Coding of effect sizes is described in detail below and sample size was coded as the total number of participants in a study. The research domain was coded as preferential consumer choice, inferential consumer choice, preferential non consumer choice, inferential non consumer choice, and risky gambles. The research domain was later recoded for the analysis of choice bias in the following way: inferential consumer choice and inferential non consumer choice were recoded as inferential choice while the other three domains were coded as preferential choice. We coded the eye tracker model as the specific name of the eye tracking equipment used in the study, e.g. Tobii T2150 or Tobii T60, since different models from the same producer vary in measurement accuracy and precision. Information on each eye tracker model's accuracy and precision was identified through the equipment producers' websites. We coded the dependent variable as the specific eye tracking metric in which an effect size was reported. We coded the independent variable as bottom-up control and top-down control, with bottom-up control divided into five dimensions, visual salience, surface size, left vs right position, central position, and set size, and top-down control divided into three dimensions, inferential viewing, preferential viewing, and choice bias. We outline these categories in detail below. 

\paragraph{Visual salience.} We coded studies as visual salience if they operationalized one or more of the known dimensions of visual salience such as color, edge density, contrast, or motion \citep{itti2000}. Some studies failed to indicate the direction of the salience manipulation, i.e. high vs. low levels of salience. In such cases, we contacted the original author and asked for clarification.

\paragraph{Surface Size.} We coded studies that manipulated the relative surface size of alternatives or attribute, e.g., small vs. large alternatives or attributes \citep{lohse1997a}. Some studies manipulated the number of product facings, i.e., the number of the same product on a supermarket shelf \citep{chandon2009a}. We coded such manipulation as a surface size manipulation. 

\paragraph{Left vs right and center position.} We coded studies that manipulated the left vs right position of alternatives or attributes in horizontal arrays as left vs right position \citep{kreplin2014a}. We coded studies that manipulated the centrality of alternative or attribute position in one or two-dimensional arrays as center position \citep[experiment 1A \& 1B in][]{atalay2012a,meissner2016a}.

\paragraph{Set size.} We coded studies as set size if they manipulated the number of alternatives or attributes in a given choice task, e.g., studying the effect of a two- vs. three-alternative choice task \citep{hong2016a}. We also coded whether the set size was manipulated at the level of the alternative or the attribute. 

\paragraph{Inferential viewing.} We coded studies on inferential viewing if they presented participants with identical stimuli under different task instructions, e.g., presented participants with the same choice alternatives under a preferential choice task vs. a healthy choice task \citep{orquin2019a}. We also coded whether the unit of analysis was at the level of the alternative or the attribute, i.e. whether AOI's contained alternatives or attributes. 

\paragraph{Preferential viewing.} We coded studies on preferential viewing if they measured the effect of preferences on eye movements. In these studies preference was either measured in an independent task (e.g. Becker-DeGroot-Marschak auction) or revealed through a choice in the choice task (i.e. chosen vs non-chosen option). We also coded whether the unit of analysis was at the level of the alternative, e.g. when participants prefer one alternative over another because it is cheaper or has a better flavor \citep{gidloef2017a}, or at the level of attributes, e.g. when price is more important than flavor \citep{meissner2016a}. 

\paragraph{Choice bias.} We coded studies as choice bias if they reported the difference in eye movements between the chosen alternative and all other (not chosen) alternatives. Studies that operationalized choice bias in specific time windows, e.g., the first 500 msec after stimulus onset or last 500 msec prior to choice \citep{shimojo2003a} were excluded. Based on the research domain we coded choice bias in two subgroups: preferential tasks where participants performed a preferential choice task, that is where participants were instructed to choose in accordance with their preferences \citep{schotter2010a} and inferential tasks where participants were instructed to choose in accordance with a predetermined goal, such as choosing the healthiest option \citep{schotter2012a}.


\subsection{Construct validity of the dependent variable}

A possible concern in meta-analyses of eye movements is that the included studies use different eye trackers, since data quality varies considerably across different eye-tracking equipment. Precision, which is the reliability of an eye tracker, can vary as much as from .005\degree root mean square in the best to .5\degree in the poorest remote eye-trackers \citep{holmqvist2015a}. Accuracy, which is the validity of an eye tracker, vary from around .4\degree to around 2\degree \citep{holmqvist2015a}. With an accuracy of 2\degree, the measured fixation, will on average fall as far as 2\degree away from the true fixation point. Simulations have shown that both accuracy and precision influence the capture rate, i.e., the percentage of eye movements correctly recorded within the boundaries of stimuli, which determines the degree of false positive and false negative observations \citep{orquin2018a,orquin2019a}. The level of false positive vs. negative fixations has been shown to influence effect sizes \citep{orquin2016a}. These differences in measurement validity across eye trackers may therefore introduce a bias in the meta-analysis of eye movements, since studies with lower accuracy and precision have lower validity, which, on average, attenuate effect sizes (Hunter \& Schmidt, 2004). To inspect whether the precision and validity of eye tracker attenuate effect sizes, and potentially correct for this, we ran a regression analysis on all included effect sizes with the absolute observed effect size correlation as the dependent variable and reported precision and accuracy of the eye tracking equipment as the independent variables. We fitted different models using a step-up approach \citep{ryoo2011model} based on Bayesian information criterion \citep{Schwarz1978}, including models with a fixed effect for the independent variable type (salience, surface size etc.). The final model included the main effect of accuracy and a random intercept grouped by study. The second-best model also included a fixed effect for independent variable type, and the estimates of the two models were comparable.

The accuracy and precision of eye trackers are highly correlated ($r = .63$), and presumably for this reason model fit did not improve when including precision. Despite analyzing across different study subgroups and other sources of noise, the results suggest that studies using eye trackers with lower levels of accuracy, on average, yield lower effect sizes as predicted by the psychometric meta-analysis methods, $\beta_0 = 0.569$, $\SE = 0.09$, $t = 6.293$, $p < .001$, $\beta_{\textrm{accuracy}} = -0.382$, $\SE = 0.158$, $t = -2.422$, $p = .018$ (Figure~\ref{fig:ET_accuracy_effectsize}). Having demonstrated that the accuracy of eye trackers attenuates effect sizes, the next step is to correct for this phenomenon. Psychometric meta-analysis offers a method for correcting the attenuating effects of artifacts, such as the lack of validity or reliability \citep{hunter2004a}. The correction involves an artifact multiplier, $a_a$, which is a measure of the expected attenuation of the true effect size $\rho$ caused by the artifacts in study $i$. The observed study effect size $\rho_0$ is a function of the true effect size and the artifact multiplier, $\rho_0 = a_a \rho$. In the case of measurement validity, the artifact multiplier is the square root of the validity of the measurement, $a_a = \sqrt{r_{yy}}$. From this calculation, it follows that the artifact multiplier, and, hence the validity of the measurement, can be obtained as $a_a = \rho_0 / \rho$ \citep{hunter2004a}. From our model, we have estimated the observed attenuated effect size, $\rho_0$, of study $i$ as $\beta_0 + \beta_1 \textrm{accuracy}$. Given perfect accuracy, i.e. accuracy takes the value zero, the expected effect size of study $i$ is equal to the intercept, $\beta_0$, which corresponds to the expected unattenuated effect size, $\rho$. From this it follows that the artifact multiplier, $a_a$, can be computed as the ratio of the attenuated effect size proportional to the unattenuated effect size:
%
\begin{equation}
\label{eq:artifact_multiplier}
a_a = \frac{\beta_0 + \beta_1 \textrm{accuracy}}{\beta_0}
\end{equation}

For example, if a study uses an eye tracker with an accuracy of $.50$, this yields an artifact multiplier equal to $(.569 - .382*.50)/.569 = .664$, meaning that studies with this level of accuracy will, on average, experience effect sizes that are $66.4\%$ of the true population effect size $\rho$. To compute the true average effect, $\rho$, we follow the psychometric meta-analysis method proposed by \cite{hunter2004a}. We first compute the unattenuated effect size correlation for each study, $r_i^u$, by dividing the Fisher transformed attenuated effect size with the artifact multiplier that corresponds to the level of the eye tracker accuracy and then applying the inverse Fisher transformation, $r_i^u = \tanh(\arctanh(r_i)/a_a)$. An issue with correlation coefficients is that effect of multiplication depends on the value of the coefficient, particularly near the boundaries (-1 and 1), Fisher transformation alleviates this issue. Then, we weight each study by its sample size and its level of validity, so that studies using low accuracy eye trackers are corrected upwards, in terms of their effect sizes and variance (Equation~\ref{eq:psychometric_rho}). A full list of eye trackers and their accuracy and precision can be found in Table~\ref{tab:eyetracker_specifications} in Appendix~\ref{appendix}.


\subsection{Multiple metrics}

Another possible concern in meta-analyses of eye movements is that studies often rely on different eye movement metrics as their dependent variable. However, to perform a meta-analysis, we need to compare studies across a common dependent variable. The many different eye-movement metrics stem from different research designs and research questions and, perhaps, also a lack of consensus about when and why to use which metrics. Many studies on bottom-up factors report fixation likelihood while studies on top-down factors often report fixation or dwell count. We focus on fixation likelihood and fixation count since they are easier to interpret than both the total fixation duration and the dwell count. The total fixation duration can, for instance, be difficult to interpret when there is a correlation between the fixation duration and the fixation count \citep{orquin2018a,orquin2019a}. The dwell count, defined as continuous fixations within same AOI without switching elsewhere, is similarly difficult to interpret if there is a correlation between the number of or the duration of fixations per dwell and the probability of a dwell. In order to inspect whether it would be meaningful to average effect sizes across different eye tracking metrics, we reviewed the identified articles for studies that reported effect sizes in multiple metrics. We identified in total $43$ studies reporting fixation likelihood along with one additional metric and $48$ studies reporting fixation count along with one additional metric. To investigate the strength of the relationship between metrics, we inspected the linearity of the relationship between fixation likelihood and fixation count against other metrics by plotting all observations (Figure~\ref{fig:metric_correction}). Since the four eye movement metrics are highly correlated, we assume that the metrics are related to the same underlying construct. 

While effect sizes expressed in different metrics are highly correlated, we should expect some differences between them. One mechanism that could lead to differences in effect size estimates between fixation likelihood and the remaining metrics is artificial dichotomization since fixation count, dwell count and total fixation duration are categorized as a binary outcome (fixated or not fixated). Artificial dichotomization of a naturally continuous variable attenuates correlations with other variables \citep{hunter2004a}. We should, therefore, expect effect sizes expressed in fixation likelihood to be somewhat smaller. Correcting for artificial dichotomization requires knowledge about the true distributional split. Since none of the included studies provide information about the true distributional split of the dichotomization and since we do not have access to all data sets, we are unable to compute the artifact multiplier as proposed by \cite{hunter2004a}. Furthermore, since the eye tracking metrics are distributed according to either zero inflated normal distribution (total fixation duration) or Poisson distribution (fixation and dwell count), no such adjustments for dichotomization currently exist. Instead, we propose an empirically derived correction factor, $a_m$, to convert effect sizes expressed in one metric to another. We propose to estimate the correction factor based on our sample of studies reporting multiple metrics, by taking the ratio of the sample size weighted means expressed in the two metrics of interest:
%
\begin{equation}
\label{eq:metrics_correction}
a_m = \frac{\arctanh \left( \frac{\sum M_i^1 N_i}{\sum N_i} \right)}{\arctanh \left( \frac{\sum M_i^2 N_i}{\sum N_i} \right)}
\end{equation}
%
where $\arctanh \left( \frac{\sum M_i N_i}{\sum N_i} \right)$ is the Fisher transformed average effect size for metric $M^1$ and $M^2$, respectively weighted by sample sizes, $N$ in study $i$. The ratio is computed on the Fisher transformed effect sizes in order to meaningfully compare ratios across the whole range of correlations. For similar reasons, the correction factor is applied to Fisher transformed effect sizes which are then transformed back with the inverse Fisher transformation: $\tanh(\arctanh(r_i)*a_m)$. The method takes advantage of the fact that effect sizes from the same study expressed in different metrics control for all factors that could influence the ratio.    

As expected, we find that effect sizes reported in fixation likelihood are on average smaller than those reported in metrics that are not artificially dichotomized, i.e. fixation count, dwell count, and total fixation duration. An effect size estimate expressed in fixation counts is, for instance, $97.2\%$ of the effect expressed in fixation likelihood. Table~\ref{tab:metric_correction} shows an overview of the correction factor $a_m$ when correcting to fixation likelihood (all bottom-up control studies) and fixation count (all top-down control studies). The correction factor is applied to each individual study effect size, but not to the study variance. When a study effect size is reported in the desired metric, e.g., fixation count for top-down studies, $a_m$ takes the value $1$.  


\subsection{Publication bias}

The relationship between effect size and its standard error in each group was inspected visually using funnel plots (Figure~\ref{fig:funnel_plots} and Figure~\ref{fig:funnel_plots_altatt}). The trim and fill method was used to take into account potential impact of any publication bias \citep{duval2000trim}. This method removes the studies with the lowest precision and corrects the effect sizes in an iterative process until a symmetric distribution about the effect size is achieved.


\subsection{Statistical analyses}

\subsubsection{Computation of effect sizes}

Effect size information was transformed into a common effect size, the Pearson’s correlation coefficient r. When multiple sources for computation of effect sizes were available, priority was given in decreasing order to other effect size measures such as eta squared, chi square, or odds ratio, means, and standard deviations, test statistics (e.g., F, t, wald), beta coefficient, or p values. For studies reporting effect sizes as correlations, no further computations were performed. If a study reported p values as a threshold value, e.g., $p < .05$, we used a conservative p value equal to .05. When studies reported effect sizes for multiple AOI's, we computed the average effect size across AOI's \citep[for a similar approach, see][]{chita2016attention}. Effect sizes were extracted from the available dependent variables. Analyses were performed in R programming language with the help of several additional libraries \citep{R2020,del2012a,datatable,ggplot2,metafor}.


\subsubsection{Weighting of effect sizes, tests of heterogeneity}

The effect sizes were analyzed with a psychometric meta-analysis following the approach in \cite{hunter2004a}. Individual effect sizes were first corrected using the metric correction factor, $a_m$, to yield a common dependent variable. Studies on bottom-up factors were corrected to fixation likelihood, and studies on top-down factors were corrected to fixation count. The psychometric meta-analysis computes the true average effect size $\rho$ based on the unattenuated correlation coefficients, $r_i^u$, weighted by sample size $n_i$, and corrected for validity by the artifact multiplier, $a_a$: 
%
\begin{equation}
\label{eq:psychometric_rho}
\rho = \frac{\sum_{i=1}^k n_i a_a^2 r_i^u}{\sum_{i=1}^k n_i a_a^2}
\end{equation}

To inspect the degree of heterogeneity in the meta-analysis, we computed the $I^2$ statistic. The $I^2$ is the proportion of variance in the observed (attenuated) effect estimates explained by artifacts and sampling error \citep{borenstein2011introduction}: 
%
\begin{equation}
\label{eq:i2_statistic}
I^2 = \frac{(T^u)^2}{(S^u)^2}
\end{equation}
%
where $(S^u)^2$ is the weighted variance of the unattenuated effect size $\rho$
%
\begin{equation}
\label{eq:Su2_var}
(S^u)^2 = \frac{\sum_{i=1}^k n_i a_a^2 (\rho_i - \hat{\rho})^2}{\sum_{i=1}^k n_i a_a^2}
\end{equation}
%
and $(T^u)^2$ is the between-studies variance component of the unattenuated effect size $\rho$
%
\begin{equation}
\label{eq:Tu2_var}
(T^u)^2 = (S^u)^2 \frac{\sum_{i=1}^k n_i a_a^2 v_i}{\sum_{i=1}^k n_i a_a^2}
\end{equation}
%
where $v_i$ is the variance of study $i$ computed as $(1 - \hat{r}^2)^2 / (n_i - 1)$ and $\hat{r}$ is the sample size weighted average effect size.


\subsection{Data availability}

The data, code used for analyzing the data and other project related files are publicly available at the Open Science Framework website: \url{https://osf.io/buk7p} \citep{osfrepo}.


% -----------------------------------------------------------
% Bibliography
% -----------------------------------------------------------

\bibliographystyle{apalike}
\bibliography{references.bib}



% -----------------------------------------------------------
% Appendix
% -----------------------------------------------------------

\FloatBarrier
\section{Appendix}
\label{appendix}


\begin{figure}
\includegraphics{ET_accuracy_effectsize}
\centering
\caption{Accuracy of the eye tracker affects the ability to reliably measure effect sizes in each study. Point denote accuracy of an eye tracker used in a study and absolute effect size (all converted to correlation coefficients) measured with it. Line is based on the estimated intercept and slope from the best fitting mixed-effect model which was used to compute artifact multiplier, $a_a$. The multiplier was used to correct for a bias in estimated effect sizes due to differences in measurement accuracy of eye trackers.}
\label{fig:ET_accuracy_effectsize}
\end{figure}
\clearpage


% \caption{Eye tracker specifications table}
% \label{tab:eyetracker_specs}
\input{./tables/eyetracker_specs.tex}
\clearpage


\begin{figure}%[H]
\includegraphics{metric_correction}
\centering
\caption{Variety of eye movement measures are used as a metric for dependent variable, but they are all highly correlated, suggesting they are all measuring the same underlying construct. Scatterplots show the relationship (A) between effect sizes expressed in fixation likelihood and fixation count, (B) between total fixation duration and fixation likelihood, (C) between total fixation duration and fixation count, (D) between total fixation duration and dwell count. Lines in each plot represent the best-fitting linear regression line, and the shaded area 95\% confidence interval.}
\label{fig:metric_correction}
\end{figure}
\clearpage


% \caption{Metric correction factor $a_m$ when correcting to either fixation count or fixation likelihood}
% \label{tab:metric_correction}
\input{./tables/metric_correction.tex}
\clearpage


% \caption{Trim and fill analysis for each independent subgroup.}
% \label{tab:trim_fill_results}
\input{./tables/trim_fill_results.tex}
\clearpage


\begin{figure}%[H]
\includegraphics{forest_plots_altatt}
\centering
\caption{Forest plots showing the unattenuated effect size correlations for each study as well as average effect across study groups. Salience A), surface size B), left vs right position C), central position D), set size for alternative E), set size for attribute F), inferential viewing for alternative G), inferential viewing for attribute H), preferential viewing for alternative I), preferential viewing for attribute J), choice bias K). Error bars represent the 95\% confidence interval of the mean.}
\label{fig:forest_plots_altatt}
\end{figure}
\clearpage


\begin{figure}%[H]
\includegraphics{funnel_plots_altatt}
\centering
\caption{Funnel plots for each study subgroup. Observations are Fishers z transformed correlation coefficients against their standard error. Asymmetric distributions of observations can indicate the presence of publication bias since smaller studies (those with higher standard errors) have more variable effect sizes and are less likely to be published unless the effect is large. Salience A), surface size B), left vs right position C), central position D), set size E), set size for alternative F), set size for attribute G), inferential viewing H), inferential viewing for alternative I), inferential viewing for attribute J), preferential viewing K), preferential viewing for alternative L), preferential viewing for attribute M), choice bias N).}
\label{fig:funnel_plots_altatt}
\end{figure}
\clearpage


\end{document}